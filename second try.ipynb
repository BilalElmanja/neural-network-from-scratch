{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd() + '\\ex3data1.mat'\n",
    "data = loadmat(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs = data['X']\n",
    "training_outputs = data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs, training_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1/(1 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(inputs , num_inputs , num_hidden , num_outputs):\n",
    "    \n",
    "    layers = [num_inputs] + num_hidden + [num_outputs]\n",
    "    \n",
    "    weights = []\n",
    "    \n",
    "    #creating random values for weights\n",
    "    for i in range(len(layers) - 1):\n",
    "        \n",
    "        w = np.random.rand(layers[i] , layers[i+1])\n",
    "        \n",
    "        weights.append(w)\n",
    "    \n",
    "    #forward propagating \n",
    "    #intilizing activations to the input values\n",
    "    activations = inputs\n",
    "    \n",
    "    #creating activations list so we can use them later\n",
    "    activations_list = [activations]\n",
    "    \n",
    "    #iterating over weights between two consecutive layers\n",
    "    for w in weights:\n",
    "        \n",
    "        #matrix multiplication between weights and activations\n",
    "        next_inputs = np.dot(activations , w)\n",
    "        \n",
    "        #calculating next activations\n",
    "        activations = sigmoid(next_inputs)\n",
    "        \n",
    "        #append activations to their list\n",
    "        activations_list.append(activations)\n",
    "        \n",
    "    return activations_list , weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing back_propagation function\n",
    "def back_propagation(target):\n",
    "    #calculating the error in the last activations\n",
    "    error = activations[-1] - target\n",
    "    \n",
    "    #calculating the derivative of the last activations\n",
    "    derivatives = []\n",
    "    for i in range(len(layers) - 1):\n",
    "        #intilizing the derivatives to zeros\n",
    "        derivative = np.zeros((layers[i] , layers[i+1]))\n",
    "        #append each derivative to the derivatives list so we can use them later\n",
    "        derivatives.append(derivative)\n",
    "        \n",
    "    # now we'll be calculating the derivatives\n",
    "    #the reversed method means that we will be starting from the end of the network (right --> left)\n",
    "    for i in reversed(range(len(derivatives))):\n",
    "        \n",
    "        print('derivation number :' , i)\n",
    "        #selecting the (i+1)th activations\n",
    "        last_activations = activations[i+1]\n",
    "        \n",
    "        #calculating the delta\n",
    "        delta = error * sigmoid(last_activations)\n",
    "        \n",
    "        #reshaping the delta to match the matrix multiplication with the ith activations\n",
    "        delta_reshaped = delta.reshape(delta.shape[0] , -1).T\n",
    "        \n",
    "        #selecting the ith activations\n",
    "        current_activations = activations[i]\n",
    "        \n",
    "        #reshaping the current activations\n",
    "        current_reshaped_activations = current_activations.reshape(current_activations.shape[0] , -1)\n",
    "        \n",
    "        #upgrading the derivatives\n",
    "        derivatives[i] = np.dot(current_reshaped_activations , delta_reshaped)\n",
    "        \n",
    "        #changing the error to calculate the next derivative\n",
    "        error = np.dot(delta , weights[i].T)\n",
    "        \n",
    "    return derivatives , error\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(learning_rate):\n",
    "    \n",
    "    #looping over the weights\n",
    "    for w in range(len(weights)):\n",
    "        weight = weights[w]\n",
    "        derivative = derivatives[w]\n",
    "        #upgrading the weights\n",
    "        weight += derivative * learning_rate\n",
    "        new_weights.append(weight)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inputs , targets , learning_rate):\n",
    "    #number of iterations\n",
    "    iterations = 100\n",
    "    errors = []\n",
    "    #looping over the number of iterations to train the model\n",
    "    for i in range(iterations):\n",
    "        #looping over the training set\n",
    "        for j in range(len(inputs)):\n",
    "            #selecting the ith training example\n",
    "            training_input = inputs[j]\n",
    "            #selecting the ith training target example\n",
    "            target = targets[j]\n",
    "            #applying forward propagation\n",
    "            activations_list , weights = forward_propagation(training_input , training_input.shape[0] , [25] , 1)\n",
    "            #applying back_propagation\n",
    "            derivatives , error = back_propagation(target)\n",
    "            #applying gradient descent\n",
    "            gradient_descent(learning_rate)\n",
    "            #adding the errors to a list so we can have a plot with matplotlib\n",
    "            errors.append(np.sum(error))\n",
    "            print('errors : ' , sum_errors)\n",
    "    \n",
    "    print('training complet successfully')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(training_inputs , training_outputs , 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
